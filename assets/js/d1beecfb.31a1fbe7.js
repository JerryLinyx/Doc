"use strict";(self.webpackChunkmy_web=self.webpackChunkmy_web||[]).push([[8941],{8126:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"LLM/Systems-Frameworks-Performance","title":"Systems-Frameworks-Performance","description":"Scaling & Model Trends","source":"@site/docs/LLM/Systems-Frameworks-Performance.md","sourceDirName":"LLM","slug":"/LLM/Systems-Frameworks-Performance","permalink":"/Doc/docs/LLM/Systems-Frameworks-Performance","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/LLM/Systems-Frameworks-Performance.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"LLM","permalink":"/Doc/docs/category/llm"},"next":{"title":"Reasoning","permalink":"/Doc/docs/LLM/Reasoning"}}');var s=r(4848),a=r(8453);const t={sidebar_position:1},o=void 0,l={},c=[{value:"Scaling &amp; Model Trends",id:"scaling--model-trends",level:2},{value:"Distributed Training &amp; Parallelism",id:"distributed-training--parallelism",level:2},{value:"Serving, Inference &amp; Memory Efficiency",id:"serving-inference--memory-efficiency",level:2},{value:"Scheduling &amp; Resource Management",id:"scheduling--resource-management",level:2},{value:"Additional References",id:"additional-references",level:2}];function h(e){const n={a:"a",em:"em",h2:"h2",li:"li",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"scaling--model-trends",children:"Scaling & Model Trends"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compute Trends across Three Eras of Machine Learning"})," \u2014 Sevilla et al., 2022. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2202.05924",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scaling Laws for Neural Language Models"})," \u2014 Kaplan et al., 2020. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2001.08361",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance"})," \u2014 Chowdhery et al., 2022. ",(0,s.jsx)(n.a,{href:"https://research.google/blog/pathways-language-model-palm-scaling-to-540-billion-parameters-for-breakthrough-performance/",children:"Blog"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLaMA: Open and Efficient Foundation Language Models"})," \u2014 Touvron et al., 2023. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2302.13971",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Power of Scale for Parameter-Efficient Prompt Tuning"})," \u2014 Lester, Al-Rfou & Constant, 2021. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2104.08691",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Best Practices and Lessons Learned on Synthetic Data for Language Models"})," \u2014 Liu et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2404.07503",children:"arXiv"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"distributed-training--parallelism",children:"Distributed Training & Parallelism"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Modeling and Workload Analysis of Distributed Large Language Model Training and Inference"})," \u2014 Kundu et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2407.14645",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient Training of Large Language Models on Distributed Infrastructures: A Survey"})," \u2014 Zhang et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2407.20018",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training"})," \u2014 Lin et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/lin-zhiqi",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale"})," \u2014 Choudhury et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/choudhury",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Metis: Fast Automatic Distributed Training on Heterogeneous GPUs"})," \u2014 Um et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/um",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accelerating the Training of Large Language Models using Efficient Activation Rematerialization and Optimal Hybrid Parallelism"})," \u2014 Yuan et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/yuan",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AMSP: Reducing Communication Overhead of ZeRO for Efficient LLM Training"})," \u2014 Zheng et al., 2023. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2311.00257",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FwdLLM: Efficient Federated Finetuning of Large Language Models with Perturbed Inferences"})," \u2014 Xu et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/xu",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ray: A Distributed Framework for Emerging AI Applications"})," \u2014 Moritz et al., OSDI 2018. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi18/presentation/moritz",children:"USENIX"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"serving-inference--memory-efficiency",children:"Serving, Inference & Memory Efficiency"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PagedAttention: Efficient Memory Management for Large Language Model Serving"})," \u2014 Kwon et al., SOSP 2023. ",(0,s.jsx)(n.a,{href:"https://dl.acm.org/doi/10.1145/3600006.3613165",children:"ACM DL"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"})," \u2014 Dao et al., NeurIPS 2022. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2205.14135",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sarathi-Serve: Taming Throughput-Latency Tradeoff in LLM Inference"})," \u2014 Agrawal et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2403.02310",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DistServe: Disaggregating Prefill and Decoding for Goodput-Optimized Large Language Model Serving"})," \u2014 Zhong et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/zhong",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llumnix: Dynamic Scheduling for Large Language Model Serving"})," \u2014 Sun et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2406.03243",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management"})," \u2014 Lee et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/lee",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ServerlessLLM: Low-Latency Serverless Inference for Large Language Models"})," \u2014 Fu et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/fu",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead"})," \u2014 Br\xfcel-Gabrielsson et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2407.00066",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost-Efficient Large Language Model Serving for Multi-Turn Conversations with CachedAttention"})," \u2014 Gao et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/gao",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quant-LLM: Accelerating the Serving of Large Language Models via FP6-Centric Algorithm-System Co-Design on Modern GPUs"})," \u2014 Xia et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/xia",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"StreamBox: A Lightweight GPU Sandbox for Serverless Inference Workflow"})," \u2014 Wu et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/wu-hao",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Compute-Optimal Inference for Problem-Solving with Language Models"})," \u2014 Wu et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2408.00724",children:"arXiv"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"scheduling--resource-management",children:"Scheduling & Resource Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Centimani: Enabling Fast AI Accelerator Selection for DNN Training with a Novel Performance Predictor"})," \u2014 Xie et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/xie",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pecan: Cost-Efficient ML Data Preprocessing with Automatic Transformation Ordering and Hybrid Placement"})," \u2014 Graur et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/graur",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MuxFlow: Efficient and Safe GPU Sharing in Large-Scale Production Deep Learning Clusters"})," \u2014 Zhao et al., 2023. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2303.13803",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caravan: Practical Online Learning of In-Network ML Models with Labeling Agents"})," \u2014 Zhang et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/zhang-qizheng",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Infrastructure Powering IBM's Gen AI Model Development"})," \u2014 Gershon et al., 2024. ",(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2407.05467",children:"arXiv"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ChameleonAPI: Automatic and Efficient Customization of Neural Networks for ML Applications"})," \u2014 Liu et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/liu",children:"USENIX"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PUZZLE: Efficiently Aligning Large Language Models through Lightweight Context Switch"})," \u2014 Lei et al., USENIX ATC 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/atc24/presentation/lei",children:"USENIX"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"additional-references",children:"Additional References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PAL: Program-Aided Language Models"})," \u2014 Gao et al., ICML 2023. ",(0,s.jsx)(n.a,{href:"https://proceedings.mlr.press/v202/gao23d.html",children:"PMLR"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"When Will My ML Job Finish? Toward Providing Completion Time Estimates through Predictability-Centric Scheduling"})," \u2014 Faisal et al., OSDI 2024. ",(0,s.jsx)(n.a,{href:"https://www.usenix.org/conference/osdi24/presentation/faisal",children:"USENIX"})]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.em,{children:"See Serving, Inference & Memory Efficiency for complementary deployment patterns such as Compress then Serve, ServerlessLLM, CachedAttention, and Quant-LLM."})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var i=r(6540);const s={},a=i.createContext(s);function t(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);