---
title: MDP <é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹>
date: '2024-04-23T19:51:05'
authors:
- jerry
tags:
- AI
- RL
- MLE
category: MLE
draft: true
---
<!-- 
# Markov Decision Process
A Markov Decision Process (MDP) is defined by: 
- A set of states, ğ‘  âˆˆ ğ’®
- A set of actions, ğ‘ âˆˆ ğ’œ
- A transition model, ğ‘ƒ(ğ‘†<sub>t+1</sub> = ğ‘ <sub>t+1</sub>|ğ‘†<sub>t</sub> = ğ‘ <sub>t</sub>, ğ‘<sub>t</sub>)
    - ğ‘†<sub>t</sub> is the **state** at time t
    - ğ‘<sub>t</sub> is the **action** taken at time t (not random)

- A **reward** function, ğ‘Ÿ(ğ‘ )

# Solving an MDP: The Policy
- Since ğ‘ƒ(ğ‘†<sub>t+1</sub> = ğ‘ <sub>t+1</sub>|ğ‘†<sub>t</sub> = ğ‘ <sub>t</sub>, ğ‘<sub>t</sub>) and ğ‘Ÿ(ğ‘ ) depend only on the state (the model is Markov), a complete solution can be expressed as follows:
- What is the best action to take in any given state?
- A **policy**, ğ‘ = ğœ‹(ğ‘ ), is a function telling you, for any state ğ‘ , what is the best action to take in that state.

# Utility
The **utility** of a state, ğ‘¢(ğ‘ ), is defined to be:
- the sum of all current and future rewards that can be achieved if we start in state ğ‘ , if we choose the best possible sequence of actions, and if we average over all possible results of those actions.

- The utility of a state, u(s), is the maximum, over all possible sequences of actions, of the expected value, over all possible results of those actions, of the total of all future rewards.  -->
